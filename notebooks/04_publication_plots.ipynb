{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88202f3f",
   "metadata": {},
   "source": [
    "# 04 — Publication Plots & Results Tables\n",
    "\n",
    "This notebook generates the key figures and tables for the CertiRAG paper.\n",
    "All plots use synthetic/demo data — replace with real eval results for submission.\n",
    "\n",
    "## Figures:\n",
    "1. **Main comparison table** — CertiRAG vs baselines on 3 benchmarks\n",
    "2. **Ablation table** — Component-level contribution analysis\n",
    "3. **Threshold sensitivity** — F1 vs τ_e heatmap\n",
    "4. **MSE compression** — Evidence reduction analysis\n",
    "5. **Calibration reliability** — Before vs after calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b87dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from certirag.utils import set_all_seeds\n",
    "\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641a5d6",
   "metadata": {},
   "source": [
    "## 1. Main Results Table\n",
    "\n",
    "Comparison of CertiRAG against baselines across three benchmarks.\n",
    "All numbers are synthetic placeholders — replace with actual eval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fcac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic results — structure matches paper format\n",
    "BENCHMARKS = [\"ALCE\", \"RAGTruth\", \"AggreFact\"]\n",
    "METHODS = [\n",
    "    \"Vanilla RAG\",\n",
    "    \"Self-RAG\",\n",
    "    \"CRAG\",\n",
    "    \"FActScore\",\n",
    "    \"CertiRAG (ours)\",\n",
    "]\n",
    "\n",
    "# Format: {method: {benchmark: {metric: value}}}\n",
    "RESULTS = {\n",
    "    \"Vanilla RAG\":     {\"ALCE\": {\"P\": 0.72, \"R\": 0.81, \"F1\": 0.76, \"Faith\": 0.68},\n",
    "                        \"RAGTruth\": {\"P\": 0.69, \"R\": 0.78, \"F1\": 0.73, \"Faith\": 0.65},\n",
    "                        \"AggreFact\": {\"P\": 0.71, \"R\": 0.80, \"F1\": 0.75, \"Faith\": 0.67}},\n",
    "    \"Self-RAG\":        {\"ALCE\": {\"P\": 0.78, \"R\": 0.76, \"F1\": 0.77, \"Faith\": 0.74},\n",
    "                        \"RAGTruth\": {\"P\": 0.76, \"R\": 0.73, \"F1\": 0.74, \"Faith\": 0.72},\n",
    "                        \"AggreFact\": {\"P\": 0.77, \"R\": 0.75, \"F1\": 0.76, \"Faith\": 0.73}},\n",
    "    \"CRAG\":            {\"ALCE\": {\"P\": 0.80, \"R\": 0.74, \"F1\": 0.77, \"Faith\": 0.76},\n",
    "                        \"RAGTruth\": {\"P\": 0.78, \"R\": 0.71, \"F1\": 0.74, \"Faith\": 0.74},\n",
    "                        \"AggreFact\": {\"P\": 0.79, \"R\": 0.73, \"F1\": 0.76, \"Faith\": 0.75}},\n",
    "    \"FActScore\":       {\"ALCE\": {\"P\": 0.82, \"R\": 0.70, \"F1\": 0.76, \"Faith\": 0.79},\n",
    "                        \"RAGTruth\": {\"P\": 0.80, \"R\": 0.68, \"F1\": 0.74, \"Faith\": 0.77},\n",
    "                        \"AggreFact\": {\"P\": 0.81, \"R\": 0.69, \"F1\": 0.75, \"Faith\": 0.78}},\n",
    "    \"CertiRAG (ours)\": {\"ALCE\": {\"P\": 0.91, \"R\": 0.82, \"F1\": 0.86, \"Faith\": 0.93},\n",
    "                        \"RAGTruth\": {\"P\": 0.89, \"R\": 0.80, \"F1\": 0.84, \"Faith\": 0.91},\n",
    "                        \"AggreFact\": {\"P\": 0.90, \"R\": 0.81, \"F1\": 0.85, \"Faith\": 0.92}},\n",
    "}\n",
    "\n",
    "# Print LaTeX-style table\n",
    "print(\"Table 1: Main results across three benchmarks (synthetic data)\\n\")\n",
    "header = f\"{'Method':<20}\"\n",
    "for bench in BENCHMARKS:\n",
    "    header += f\" | {'P':>5} {'R':>5} {'F1':>5} {'Fth':>5}\"\n",
    "print(header)\n",
    "print(\"=\" * len(header))\n",
    "\n",
    "for method in METHODS:\n",
    "    row = f\"{method:<20}\"\n",
    "    for bench in BENCHMARKS:\n",
    "        r = RESULTS[method][bench]\n",
    "        bold = method == \"CertiRAG (ours)\"\n",
    "        fmt = lambda v: f\"**{v:.2f}**\" if bold else f\"{v:.2f}\"\n",
    "        row += f\" | {fmt(r['P']):>7} {fmt(r['R']):>7} {fmt(r['F1']):>7} {fmt(r['Faith']):>7}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f4f6c",
   "metadata": {},
   "source": [
    "## 2. Ablation Study Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABLATIONS = {\n",
    "    \"Full CertiRAG\":        {\"F1\": 0.86, \"Faith\": 0.93, \"MSE_ratio\": 0.34, \"Latency_ms\": 420},\n",
    "    \"− MSE selection\":      {\"F1\": 0.85, \"Faith\": 0.91, \"MSE_ratio\": 1.00, \"Latency_ms\": 380},\n",
    "    \"− Claim normalisation\":{\"F1\": 0.82, \"Faith\": 0.89, \"MSE_ratio\": 0.38, \"Latency_ms\": 400},\n",
    "    \"− Cross-encoder\":      {\"F1\": 0.83, \"Faith\": 0.90, \"MSE_ratio\": 0.36, \"Latency_ms\": 310},\n",
    "    \"BM25-only retrieval\":  {\"F1\": 0.79, \"Faith\": 0.87, \"MSE_ratio\": 0.41, \"Latency_ms\": 280},\n",
    "    \"Dense-only retrieval\": {\"F1\": 0.81, \"Faith\": 0.88, \"MSE_ratio\": 0.39, \"Latency_ms\": 350},\n",
    "    \"NLI verifier\":         {\"F1\": 0.80, \"Faith\": 0.86, \"MSE_ratio\": 0.37, \"Latency_ms\": 390},\n",
    "    \"LLM-judge verifier\":   {\"F1\": 0.84, \"Faith\": 0.90, \"MSE_ratio\": 0.35, \"Latency_ms\": 1200},\n",
    "}\n",
    "\n",
    "print(\"Table 2: Ablation study on ALCE benchmark (synthetic data)\\n\")\n",
    "print(f\"{'Configuration':<25} {'F1':>6} {'Faith':>7} {'MSE↓':>6} {'Lat(ms)':>8}\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "for config_name, metrics in ABLATIONS.items():\n",
    "    marker = \" ★\" if config_name == \"Full CertiRAG\" else \"\"\n",
    "    print(f\"{config_name:<25} {metrics['F1']:>6.2f} {metrics['Faith']:>7.2f} \"\n",
    "          f\"{metrics['MSE_ratio']:>6.2f} {metrics['Latency_ms']:>8d}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24666f5",
   "metadata": {},
   "source": [
    "## 3. Threshold sensitivity heatmap (text-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b90cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate F1 scores for threshold grid\n",
    "tau_e_vals = np.arange(0.50, 0.96, 0.05)\n",
    "tau_c_vals = np.arange(0.40, 0.86, 0.05)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Generate plausible F1 surface: peaks around τ_e≈0.85, τ_c≈0.70\n",
    "def f1_surface(te, tc):\n",
    "    base = 0.86\n",
    "    # Penalty for deviation from optimal\n",
    "    penalty_e = -2.0 * (te - 0.85)**2\n",
    "    penalty_c = -1.5 * (tc - 0.70)**2\n",
    "    # Interaction term\n",
    "    interaction = -0.5 * abs(te - tc - 0.15)\n",
    "    noise = rng.normal(0, 0.005)\n",
    "    return np.clip(base + penalty_e + penalty_c + interaction + noise, 0.50, 0.90)\n",
    "\n",
    "print(\"Figure 3: F1 vs (τ_e, τ_c) threshold grid (synthetic)\\n\")\n",
    "print(f\"{'':>8}\", end=\"\")\n",
    "for tc in tau_c_vals:\n",
    "    print(f\" τ_c={tc:.2f}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (8 + len(tau_c_vals) * 10))\n",
    "\n",
    "for te in tau_e_vals:\n",
    "    print(f\"τ_e={te:.2f}\", end=\" \")\n",
    "    for tc in tau_c_vals:\n",
    "        f1 = f1_surface(te, tc)\n",
    "        # Use unicode blocks for heatmap effect\n",
    "        if f1 >= 0.84:\n",
    "            sym = \"█\"\n",
    "        elif f1 >= 0.80:\n",
    "            sym = \"▓\"\n",
    "        elif f1 >= 0.75:\n",
    "            sym = \"▒\"\n",
    "        else:\n",
    "            sym = \"░\"\n",
    "        print(f\"  {sym}{f1:.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nLegend: █≥0.84  ▓≥0.80  ▒≥0.75  ░<0.75\")\n",
    "print(f\"Optimal region: τ_e∈[0.80,0.90], τ_c∈[0.65,0.75]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b28e0",
   "metadata": {},
   "source": [
    "## 4. MSE compression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate MSE compression ratios per claim\n",
    "N_CLAIMS = 50\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "total_evidence = rng.integers(3, 15, size=N_CLAIMS)\n",
    "mse_selected = np.array([max(1, int(t * rng.beta(2, 5))) for t in total_evidence])\n",
    "compression = mse_selected / total_evidence\n",
    "\n",
    "print(\"Figure 4: MSE Evidence Compression\\n\")\n",
    "print(f\"Claims analysed: {N_CLAIMS}\")\n",
    "print(f\"Mean total evidence per claim:  {total_evidence.mean():.1f}\")\n",
    "print(f\"Mean MSE selected per claim:    {mse_selected.mean():.1f}\")\n",
    "print(f\"Mean compression ratio:         {compression.mean():.3f}\")\n",
    "print(f\"Median compression ratio:       {np.median(compression):.3f}\")\n",
    "\n",
    "# Histogram (text-based)\n",
    "print(f\"\\nCompression ratio distribution:\")\n",
    "bins = np.arange(0, 1.05, 0.1)\n",
    "hist, _ = np.histogram(compression, bins=bins)\n",
    "max_count = max(hist)\n",
    "for i in range(len(hist)):\n",
    "    bar = \"█\" * int(hist[i] / max_count * 40) if max_count > 0 else \"\"\n",
    "    print(f\"  [{bins[i]:.1f}-{bins[i+1]:.1f}) {bar} {hist[i]}\")\n",
    "\n",
    "print(f\"\\n→ MSE reduces evidence by {(1-compression.mean())*100:.0f}% on average\")\n",
    "print(f\"  while maintaining verification guarantees (Theorem 1).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90d8bb",
   "metadata": {},
   "source": [
    "## 5. Verifier comparison across benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a52c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERIFIER_RESULTS = {\n",
    "    \"MiniCheck\": {\n",
    "        \"ALCE\": {\"F1\": 0.86, \"Faith\": 0.93, \"ECE\": 0.032},\n",
    "        \"RAGTruth\": {\"F1\": 0.84, \"Faith\": 0.91, \"ECE\": 0.035},\n",
    "        \"AggreFact\": {\"F1\": 0.85, \"Faith\": 0.92, \"ECE\": 0.029},\n",
    "    },\n",
    "    \"NLI-DeBERTa\": {\n",
    "        \"ALCE\": {\"F1\": 0.80, \"Faith\": 0.86, \"ECE\": 0.058},\n",
    "        \"RAGTruth\": {\"F1\": 0.78, \"Faith\": 0.84, \"ECE\": 0.062},\n",
    "        \"AggreFact\": {\"F1\": 0.79, \"Faith\": 0.85, \"ECE\": 0.055},\n",
    "    },\n",
    "    \"LLM-Judge\": {\n",
    "        \"ALCE\": {\"F1\": 0.84, \"Faith\": 0.90, \"ECE\": 0.041},\n",
    "        \"RAGTruth\": {\"F1\": 0.82, \"Faith\": 0.88, \"ECE\": 0.045},\n",
    "        \"AggreFact\": {\"F1\": 0.83, \"Faith\": 0.89, \"ECE\": 0.038},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Table 3: Verifier comparison (synthetic data)\\n\")\n",
    "for bench in BENCHMARKS:\n",
    "    print(f\"\\n--- {bench} ---\")\n",
    "    print(f\"{'Verifier':<15} {'F1':>6} {'Faith':>7} {'ECE':>6}\")\n",
    "    print(\"-\" * 36)\n",
    "    for verifier, results in VERIFIER_RESULTS.items():\n",
    "        r = results[bench]\n",
    "        best = verifier == \"MiniCheck\"\n",
    "        marker = \" ★\" if best else \"\"\n",
    "        print(f\"{verifier:<15} {r['F1']:>6.2f} {r['Faith']:>7.2f} {r['ECE']:>6.3f}{marker}\")\n",
    "\n",
    "print(\"\\n★ = Best performer\")\n",
    "print(\"\\nKey insight: MiniCheck achieves highest F1 and faithfulness with\")\n",
    "print(\"lowest calibration error, validating it as the default verifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a5f70",
   "metadata": {},
   "source": [
    "## 6. LaTeX table export\n",
    "\n",
    "Generate copy-paste-ready LaTeX for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca901d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latex_main_table():\n",
    "    \"\"\"Generate LaTeX for the main results table.\"\"\"\n",
    "    lines = [\n",
    "        r\"\\begin{table}[t]\",\n",
    "        r\"\\centering\",\n",
    "        r\"\\caption{Main results on three hallucination benchmarks.}\",\n",
    "        r\"\\label{tab:main-results}\",\n",
    "        r\"\\resizebox{\\textwidth}{!}{%\",\n",
    "        r\"\\begin{tabular}{l\" + \"cccc\" * len(BENCHMARKS) + \"}\",\n",
    "        r\"\\toprule\",\n",
    "    ]\n",
    "    \n",
    "    # Header rows\n",
    "    header1 = r\"\\multirow{2}{*}{Method}\"\n",
    "    for bench in BENCHMARKS:\n",
    "        header1 += f\" & \\\\multicolumn{{4}}{{c}}{{{bench}}}\"\n",
    "    header1 += r\" \\\\\"\n",
    "    lines.append(header1)\n",
    "    \n",
    "    header2 = \"\"\n",
    "    for _ in BENCHMARKS:\n",
    "        header2 += r\" & P & R & F1 & Faith\"\n",
    "    header2 += r\" \\\\\"\n",
    "    lines.append(r\"\\cmidrule(lr){2-5}\" * len(BENCHMARKS))\n",
    "    lines.append(header2)\n",
    "    lines.append(r\"\\midrule\")\n",
    "    \n",
    "    # Data rows\n",
    "    for method in METHODS:\n",
    "        is_ours = method == \"CertiRAG (ours)\"\n",
    "        row = method.replace(\"(\", \"\\\\textbf{(\").replace(\")\", \")}\") if is_ours else method\n",
    "        for bench in BENCHMARKS:\n",
    "            r = RESULTS[method][bench]\n",
    "            for metric in [\"P\", \"R\", \"F1\", \"Faith\"]:\n",
    "                val = f\"{r[metric]:.2f}\"\n",
    "                if is_ours:\n",
    "                    val = f\"\\\\textbf{{{val}}}\"\n",
    "                row += f\" & {val}\"\n",
    "        row += r\" \\\\\"\n",
    "        lines.append(row)\n",
    "    \n",
    "    lines.extend([\n",
    "        r\"\\bottomrule\",\n",
    "        r\"\\end{tabular}}\",\n",
    "        r\"\\end{table}\",\n",
    "    ])\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "latex = to_latex_main_table()\n",
    "print(\"LaTeX output (copy to paper):\\n\")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae28f0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes for Paper Submission\n",
    "\n",
    "1. Replace all synthetic data with actual eval runner output\n",
    "2. Run `make eval` to generate real metrics on all benchmarks\n",
    "3. Use `eval/plots.py` for matplotlib/seaborn publication-quality figures\n",
    "4. Ensure all numbers match between paper text, tables, and figures\n",
    "5. Include confidence intervals (bootstrap) for final submission"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
